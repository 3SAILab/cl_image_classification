# AlexNet
### 一、网络架构
![Alt](https://i-blog.csdnimg.cn/direct/378bf6d10c5944648c802813ada462f0.png)  
8个学习层：5个卷积层+3个全连接层（部分卷积层后有最大池化）

### 二、重点
#### 1.ReLU做激活函数 $\color{green}{\checkmark}$
- 传统tanh和sigmoid函数容易饱和，训练慢。而ReLU是max(0,x) (输入正输出x,负输出0)。
- ReLU输入中的负值指该特征与所学特征负相关。
- 主要好处是：  
①防止梯度消失，因为在x>0的时候梯度恒为1，深层训练更稳定  
②计算快，只用判断输入是否为正就好  
③降低过拟合风险，负值都输出0，相当于失活部分神经元，使模型不用学习许多不必要的特征
#### 2.多GPU训练
- 把网络拆到两个GPU上以进行大模型训练。  
- ps1: pytorch官方未实现，技术过于复杂通常不用。  
#### 3.LRN局部响应归一化
- 每个神经元的输出会被周围神经元的输出归一化，最终强的突出，弱的抑制。目的是防止过拟合，增强模型泛化能力。  
公式：  
$$
b_{x,y}^i = \frac{a_{x,y}^i}{\left(k+\alpha\sum_{j=\max(0,i-\frac{n}{2})}^{\min(N-1,i+\frac{n}{2})}(a_{x,y}^j)^2\right)^\beta}
$$
- ps:作用有限，基本不用，pytorch官方未实现  
#### 4.重叠池化 $\color{green}{\checkmark}$
- 用kernel_size=3,stride=2的池化层，使相邻池化区域重叠（因为kernel_size>stride），减少过拟合。  
#### 5.Dropout防止过拟合 $\color{green}{\checkmark}$
- 全连接层训练时，使部分神经元随机失活，文章以及pytorch官方都是p=0.5。
### 三、注意点
#### 1.初始化权重
- 文章中使用均值=0，方差=0.01的高斯随机变量，bias在2，4，5卷积层=1，其余=0。

- pytorch官方使用kaiming_uniform_(均匀分布初始化)+bais均匀初始化（ps:最新版AlexNet中只有导入预训练权重的代码，随机初始化权重在nn.Conv和nn.Linear中，目录是~\anaconda3\envs\env_name\Lib\site-packages\torch\nn\modules）。

- 学习视频中是自定义随机初始化权重，即kaiming_normol_（正态分布初始化）+bais=0，会覆盖官方的。  
#### 2.优化器
- 文章中采用SGD+momentum,每一层固定学习率0.01，效果不好再手动调整，按10倍往下降。
- 学习视频中用Adam，对学习率敏感度不高，自适应学习率，不用调很多参数，收敛速度快。
# VGGNet
### 一、网络架构及配置
架构：
![Alt](https://i-blog.csdnimg.cn/direct/4611d9f33f53459aa1c49d365019589d.png)
输入：224 x 224 的RGB图像，减去平均RGB值  
卷积层：仅由 3 x 3 和 1 x 1 构成  
最大池化层：5个  
全连接层：3个  

配置：
![Alt](https://i-blog.csdnimg.cn/direct/44fa5e74cef34c34834635fd27ae73eb.png)

### 二、重点
#### 1.感受野
- 概念：一个神经元能看到的输入区域。  
公式：  
$$
F(i)=(F(i+1)-1)*Stride+Ksize
$$
$F(i)$为第$i$层感受野，$Stride$为第$i$层步距，$Ksize$为卷积核大小或池化核大小。 
![Alt](https://i-blog.csdnimg.cn/direct/cc38347d64e94149bfc4b05e0b113eaf.png) 
感受野计算是从最后一层往前推，看能在原图上“看”到多大范围。  
Feature map: F=1   
Conv3x3(3): F=(1-1)x1+3=3  
Conv3x3(2): F=(3-1)x1+3=5  
Conv3x3(1): F=(5-1)x1+3=7  
Conv7x7(1): F=(1-1)x1+7=7  
由此可知用3x3卷积核叠3层后与7x7卷积核感受野相同。
#### 2.小卷积核代替大卷积核好处
- ①同样感受野的情况下参数减少，计算更高效。  
②小卷积需要增加深度来达到相应感受野，而深度增加便于学习到更复杂的特征。  
③由于叠了多个卷积层，ReLU更多，而ReLU是有助于提高模型的判别能力的。  
### 三、注意点
#### 1.初始化权重
- 文章中首先使用较浅层的A网络随机初始化权重进行训练，然后用它的前四个卷积层和后三个全连接层的权重初始化后面的网络，中间则仍用随机。  
但文章发布后他们使用了Xavier初始化，相关论文：<https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>  
它的核心是平衡各层方差，缓解梯度消失或爆炸。  
- pytorch官方的卷积层用了kaiming_normal_初始化权重，其余都设为常量，这个随机初始化权重的函数直接写在vgg类中。
#### 2.测试时全连接层替换为卷积层
- 全连接层需要固定输入大小，因此需要批量剪裁图片，替换为卷积层后省去这一操作，提高测试速度。同时卷积操作也可以提取更多特征信息。  
#### 3.网络实现 
- 文章中一共有5种网络结构A-E，pytorch官方中未实现C，同时加入了带BatchNorm的版本，BN对每一层的输入进行归一化，相关论文：<https://arxiv.org/pdf/1502.03167>
# GoogLeNet  
### 一、网络架构
![Alt](https://i-blog.csdnimg.cn/blog_migrate/6ed5a4d39d44ea256f0a34c4e555a814.png)  
### 二、重点  
#### 1.Inception模块
- 组合不同尺寸的卷积提取多种特征，最后拼接，能够提取到更多尺度的特征。
- 其中大尺寸卷积过多可能导致计算量爆炸，因此引入1x1卷积降维，在大卷积之前较少输入通道数。  
#### 2.平均池化代替全连接层
- 文章中说引用这篇论文的结论：<https://arxiv.org/pdf/1312.4400>  
在这里应该主要是为了减少过拟合风险，全连接层需要大量训练参数，而平均池化层不用。
#### 3.辅助分类器
- 由于网络深度较大，可能导致梯度消失，无法高效的进行反向梯度传播，因此在中间加上两个辅助分类器，使浅层的网络也能接受较强信号来训练。
- 提供额外的正则化，防止过拟合。
### 三、注意点
#### 1.优化器
- 文章中用0.9动量的异步随机梯度下降（将任务分配到不同节点，个节点独立工作），每8个epoch降低4%，数据集不大时还是用Adam。
# ResNet
### 0.前期工作
#### 1.解决什么问题  
- 梯度消失和梯度爆炸 -> ReLU,BN/降低学习率等
- 退化问题 -> 残差学习
#### 2.什么亮点
- 1000层的网络结构
- residual模块（残差块）
- 使用BN
#### 3.搞懂
- shortcut connections（快捷链接）是什么？ $\color{green}{\checkmark}$  
绕过多层变换，直接加到后续层，使其不在深层中丢失
- 残差学习与残差块是什么？ $\color{green}{\checkmark}$  
核心思想是由传统的直接映射变为残差映射，原映射变为F(x)=H(x)+x。  
普通网络让每一层学H(x)，残差网络让它学与输入x的差值，就是在原有基础上优化，最差就是不进步，这样训练也能更快。
- BN是什么？ $\color{green}{\checkmark}$  
对每一层的输入数据做标准化处理，使均值为0，方差为1，让数据分布更稳定，resnet中在卷积层后，激活函数前。
- 迁移学习是什么？ $\color{green}{\checkmark}$  
将训练好的网络中浅层网络的参数迁移到新网络中。  
迁移学习方法：   
①载入权重后训练所有参数  
②载入权重后只训练最后几层参数  
③载入权重后在原网络基础上再添加一层全连接层，仅训练最后一个全连接层
- 为什么不用bias？（BN相关） $\color{green}{\checkmark}$  
BN中本身就包含了平移参数，与bias的作用类似。
### 一、网络架构  
![Alt](https://i-blog.csdnimg.cn/blog_migrate/75a8be53f53b21a4b5fa433fdf8e9cd6.png)  
从左到右分别是VGG-19,普通34层网络，34层残差网络（虚线是当维度不匹配的时候，用另一种映射方式）  
![Alt](https://i-blog.csdnimg.cn/direct/c60d5f0a4e964994b524d9008b5f57ce.png)  
左边是18和34层用的残差结构，右边是50，101，152层的  
配置：
![Alt](https://i-blog.csdnimg.cn/blog_migrate/fc311cfc3719e005c75aa728f8913e3e.png)
### 二、重点
#### 1.shortcut connection
- 绕过一层或多层变换，直接加到后续层，使其不在深层中丢失。
- 主要有3种方式：恒等、投影（1x1卷积实现）、零填充。后两个可以解决维度不匹配的问题。实验证明恒等+零填充足够解决问题，虽然全部用投影效果略好但好的不多，且参数过多。
#### 2.**深度残差学习**框架解决退化问题
- 核心思想是由传统的直接映射变为残差映射，原映射变为F(x)=H(x)+x。  
普通网络让每一层学H(x)，残差网络让它学与输入x的差值，就是在原有基础上优化，最差就是不进步，这样训练也能更快。
#### 3.两种残差块设计
- 基础块（BasicBlock）和瓶颈块（Bottleneck），前者用于18、34层，后者用于50、101、152层，主要是在深层训练时参数过多，用1x1卷积降维然后再升维可以较少计算量。
#### 4.使用BN（批归一化）
- 在深层网络训练时，由于学习到的权重等参数改变，每一层的输入分布都会改变，因此每一层都要重新适应新的分布，这不仅降低了效率，还不稳定。
- BN对每一层的输入数据做标准化处理，使均值为0，方差为1，让数据分布更稳定，一般放在卷积层后，激活函数前。
- 主要好处是：  
①训练速度加快。
②调参变简单，对权重初始化的要求降低，由于公式内部已有偏移量，因此不需要设置bias。
③起到一定程度上正则化作用，减少过拟合，可以去掉dropout。
### 三、注意点  
#### 1.Bottlenect差异
- 文章中1x1卷积stride=2,3x3卷积stride=1，pytorch官方用1x1卷积stride=1，3x3卷积stride=2，这样能在top-1上提升0.5%的准确率，官方文档：<https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch>