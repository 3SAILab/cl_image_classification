# AlexNet
### 一、网络架构
![Alt](https://i-blog.csdnimg.cn/direct/378bf6d10c5944648c802813ada462f0.png)  
8个学习层：5个卷积层+3个全连接层（部分卷积层后有最大池化）

### 二、重点
#### 1.ReLU做激活函数 $\color{green}{\checkmark}$
- 传统tanh和sigmoid函数容易饱和，训练慢。而ReLU是max(0,x) (输入正输出x,负输出0)。
- ReLU输入中的负值指该特征与所学特征负相关。
- 主要好处是：  
①防止梯度消失，因为在x>0的时候梯度恒为1，深层训练更稳定  
②计算快，只用判断输入是否为正就好  
③降低过拟合风险，负值都输出0，相当于失活部分神经元，使模型不用学习许多不必要的特征
#### 2.多GPU训练
- 把网络拆到两个GPU上以进行大模型训练。  
- ps1: pytorch官方未实现，技术过于复杂通常不用。  
#### 3.LRN局部响应归一化
- 每个神经元的输出会被周围神经元的输出归一化，最终强的突出，弱的抑制。目的是防止过拟合，增强模型泛化能力。  
公式：  
$$
b_{x,y}^i = \frac{a_{x,y}^i}{\left(k+\alpha\sum_{j=\max(0,i-\frac{n}{2})}^{\min(N-1,i+\frac{n}{2})}(a_{x,y}^j)^2\right)^\beta}
$$
- ps:作用有限，基本不用，pytorch官方未实现  
#### 4.重叠池化 $\color{green}{\checkmark}$
- 用kernel_size=3,stride=2的池化层，使相邻池化区域重叠（因为kernel_size>stride），减少过拟合。  
#### 5.Dropout防止过拟合 $\color{green}{\checkmark}$
- 全连接层训练时，使部分神经元随机失活，文章以及pytorch官方都是p=0.5。
### 三、注意点
#### 1.初始化权重
- 文章中使用均值=0，方差=0.01的高斯随机变量，bias在2，4，5卷积层=1，其余=0。

- pytorch官方使用kaiming_uniform_(均匀分布初始化)+bais均匀初始化（ps:最新版AlexNet中只有导入预训练权重的代码，随机初始化权重在nn.Conv和nn.Linear中，目录是~\anaconda3\envs\env_name\Lib\site-packages\torch\nn\modules）。

- 学习视频中是自定义随机初始化权重，即kaiming_normol_（正态分布初始化）+bais=0，会覆盖官方的。  
#### 2.优化器
- 文章中采用SGD+momentum,每一层固定学习率0.01，效果不好再手动调整，按10倍往下降。
- 学习视频中用Adam，对学习率敏感度不高，自适应学习率，不用调很多参数，收敛速度快。
# VGGNet
### 一、网络架构及配置
架构：
![Alt](https://i-blog.csdnimg.cn/direct/4611d9f33f53459aa1c49d365019589d.png)
输入：224 x 224 的RGB图像，减去平均RGB值  
卷积层：仅由 3 x 3 和 1 x 1 构成  
最大池化层：5个  
全连接层：3个  

配置：
![Alt](https://i-blog.csdnimg.cn/direct/44fa5e74cef34c34834635fd27ae73eb.png)

### 二、重点
#### 1.感受野
- 概念：一个神经元能看到的输入区域。  
公式：  
$$
F(i)=(F(i+1)-1)*Stride+Ksize
$$
$F(i)$为第$i$层感受野，$Stride$为第$i$层步距，$Ksize$为卷积核大小或池化核大小。 
![Alt](https://i-blog.csdnimg.cn/direct/cc38347d64e94149bfc4b05e0b113eaf.png) 
感受野计算是从最后一层往前推，看能在原图上“看”到多大范围。  
Feature map: F=1   
Conv3x3(3): F=(1-1)x1+3=3  
Conv3x3(2): F=(3-1)x1+3=5  
Conv3x3(1): F=(5-1)x1+3=7  
Conv7x7(1): F=(1-1)x1+7=7  
由此可知用3x3卷积核叠3层后与7x7卷积核感受野相同。
#### 2.小卷积核代替大卷积核好处
- ①同样感受野的情况下参数减少，计算更高效。  
②小卷积需要增加深度来达到相应感受野，而深度增加便于学习到更复杂的特征。  
③由于叠了多个卷积层，ReLU更多，而ReLU是有助于提高模型的判别能力的。  
### 三、注意点
#### 1.初始化权重
- 文章中首先使用较浅层的A网络随机初始化权重进行训练，然后用它的前四个卷积层和后三个全连接层的权重初始化后面的网络，中间则仍用随机。  
但文章发布后他们使用了Xavier初始化，相关论文：<https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>  
它的核心是平衡各层方差，缓解梯度消失或爆炸。  
- pytorch官方的卷积层用了kaiming_normal_初始化权重，其余都设为常量，这个随机初始化权重的函数直接写在vgg类中。
#### 2.测试时全连接层替换为卷积层
- 全连接层需要固定输入大小，因此需要批量剪裁图片，替换为卷积层后省去这一操作，提高测试速度。同时卷积操作也可以提取更多特征信息。  
#### 3.网络实现 
- 文章中一共有5种网络结构A-E，pytorch官方中未实现C，同时加入了带BatchNorm的版本，BN对每一层的输入进行归一化，相关论文：<https://arxiv.org/pdf/1502.03167>
# GoogLeNet/Inception_v1  
### 一、网络架构
![Alt](https://i-blog.csdnimg.cn/blog_migrate/6ed5a4d39d44ea256f0a34c4e555a814.png)  
### 二、重点  
#### 1.Inception模块
- 组合不同尺寸的卷积提取多种特征，最后拼接，能够提取到更多尺度的特征。
- 其中大尺寸卷积过多可能导致计算量爆炸，因此引入1x1卷积降维，在大卷积之前较少输入通道数。  
#### 2.平均池化代替全连接层
- 文章中说引用这篇论文的结论：<https://arxiv.org/pdf/1312.4400>  
在这里应该主要是为了减少过拟合风险，全连接层需要大量训练参数，而平均池化层不用。
#### 3.辅助分类器
- 由于网络深度较大，可能导致梯度消失，无法高效的进行反向梯度传播，因此在中间加上两个辅助分类器，使浅层的网络也能接受较强信号来训练。
- 提供额外的正则化，防止过拟合。
### 三、注意点
#### 1.优化器
- 文章中用0.9动量的异步随机梯度下降（将任务分配到不同节点，个节点独立工作），每8个epoch降低4%，数据集不大时还是用Adam。
# Inception_v2
### 一、改进点
#### 1.提出Batch Normalization（批标准化）方法
- 在深层网络训练时，由于学习到的权重等参数改变，每一层的输入分布都会改变，因此每一层都要重新适应新的分布，这不仅降低了效率，还不稳定。
- BN对每一层的输入数据做标准化处理，使均值为0，方差为1，让数据分布更稳定，同时加入两个可学习参数，让网络能灵活调整这个分布。
- 主要好处是：  
①训练速度加快。  
②调参变简单，对权重初始化的要求降低，由于公式内部已有偏移量，因此不需要设置bias。  
③起到一定程度上正则化作用，减少过拟合，可以去掉dropout。  
④能运用饱和激活函数如sigmoid且不会引起梯度消失。
#### 2.5x5卷积替换为两个3x3卷积
- 减少参数  
# ResNet
### 一、网络架构  
![Alt](https://i-blog.csdnimg.cn/blog_migrate/75a8be53f53b21a4b5fa433fdf8e9cd6.png)  
从左到右分别是VGG-19,普通34层网络，34层残差网络（虚线是当维度不匹配的时候，用另一种映射方式）  
![Alt](https://i-blog.csdnimg.cn/direct/c60d5f0a4e964994b524d9008b5f57ce.png)  
左边是18和34层用的残差结构，右边是50，101，152层的  
配置：
![Alt](https://i-blog.csdnimg.cn/blog_migrate/fc311cfc3719e005c75aa728f8913e3e.png)
### 二、重点
#### 1.shortcut connection
- 绕过一层或多层变换，直接加到后续层，使其不在深层中丢失。
- 主要有3种方式：恒等、投影（1x1卷积实现）、零填充。后两个可以解决维度不匹配的问题。实验证明恒等+零填充足够解决问题，虽然全部用投影效果略好但好的不多，且参数过多。
#### 2.**深度残差学习**框架解决退化问题
- 核心思想是由传统的直接映射变为残差映射，原映射变为F(x)=H(x)+x。  
普通网络让每一层学H(x)，残差网络让它学与输入x的差值，就是在原有基础上优化，最差就是不进步，这样训练也能更快。
#### 3.两种残差块设计
- 基础块（BasicBlock）和瓶颈块（Bottleneck），前者用于18、34层，后者用于50、101、152层，主要是在深层训练时参数过多，用1x1卷积降维然后再升维可以较少计算量。
#### 4.使用BN（批归一化）
- 加速训练，简化调参，一定程度正则化效果。
### 三、注意点  
#### 1.Bottlenect差异
- 文章中1x1卷积stride=2,3x3卷积stride=1，pytorch官方用1x1卷积stride=1，3x3卷积stride=2，这样能在top-1上提升0.5%的准确率，官方文档：<https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch>
# Inception_v3
### 一、改进点
#### 1.分解卷积
- 大卷积分解为小卷积，例如5x5卷积分为两个3x3的。  
- 不对称卷积分解，例如7x7变为1x7和7x1。  
更加节省计算成本，使用特征图尺寸为12x12到20x20，早期层表现较差。
#### 2.引入LSR（标签平滑正则化）
- 一种正则化手段，不让模型过度自信。
- 公式：  
$$
y_{\text{smooth}}(k) = 
\begin{cases}
1 - \epsilon & \text{if } k = \text{true label} \\
\frac{\epsilon}{K - 1} & \text{otherwise}
\end{cases}
$$

其中：
$ \epsilon $：平滑系数（如 $ 0.1 $）
$ K $：总类别数
- 例如one-hot编码后[0,0,1,0],使用LSR后变成[0.025,0.025,0.9,0.025],可以较少过拟合，提升泛化能力。
#### 3.多种inception模块
- 根据不同层级的特征图优化
#### 4.优化辅助分类器
### 二、补充
#### 1.pytorch官方将inception_v2和v3结合实现。
#### 2.googlenet及inception3对比
- 用pytorch官方的代码，修改部分参数以适配224x224输入图像，固定seed=42训练，最终inception3准确率提升3.5%，训练时间延长，参数量翻倍（判断是网络更深，结构更复杂的缘故）
#### 3.有无LSR对比
- 用googlenet模型，固定seed=42，有LSR比没有准确率提升1.6%。
# Inception_v4
### 一、改进点
#### 1.实现更简洁的Inception_v4架构（无残差）
#### 2.inception架构与残差连接结合
- 用更轻量的inception块，后接1x1卷积扩展维度，用来匹配输入维度与残差相加。
- 设计了v1,v2两个版本，前者成本与v3相当，后者与v4相当。两者主要是模块结构的不同，经过实验v2表现更好。
#### 3.残差缩放
- 当网络中卷积核的数量超过1000时，容易出现训练不稳定，在最后全局池化层之前出现很多0，且无法通过降低学习率及增加额外批归一化来解决。
- 在将残差添加到前一层激活值之前，对残差进行缩放（乘一个常数，一般在0.1-0.3），可以有效稳定训练过程。主要是通过调整残差信号的强度，防止它过大或者过小来稳定深层训练。
# SqueezeNet
### 一、网络架构
![Alt](https://i-blog.csdnimg.cn/blog_migrate/87b844cb635931a262be3f63316fe3c6.png)
从左到右是完善过程，中间加了shortcut，右边加了匹配不同维度的shortcut。
### 二、重点
#### 1.压缩策略
- 大多数网络都在往深而精改进，但现实生活中由于计算平台的限制，无法提供足够的算力支持，因此想要构架一种轻量的小型网络，用于：  
①更有效的分布式训练。  
②可以将新模型导出到客户端时开销更小。  
③可行的嵌入式部署。
- 使用3种策略：  
①将3x3卷积替换成1x1卷积。  
②减少3x3卷积的通道数。  
③下采样后置，使卷积层具有较大的activation maps，提升精度，但会增加计算量。
#### 2.fire模块（2部分组成）
- squeeze部分，一组连续的1x1卷积。限制后续3x3卷积输入通道数量。
- expand部分，一组连续的1x1卷积和一组连续的3x3卷积拼接组成。
#### 3.无全连接层
### 三、注意点
#### 1.pytorch官方复现squeezenet有两个版本，1_0第一层卷积是7x7，1_1变为3x3,并且改动部分池化层的位置。
# Wide ResNet
### 一、网络架构
![Alt](https://i-blog.csdnimg.cn/blog_migrate/20de119b10593a7960252ea739a26c13.png)
左边两个是resnet用的残差块，右边两个是本模型用的残差块。
### 二、重点
#### 1.优选残差块
- 不使用bottleneck,因为会加大网络深度。文章中经过实验选用了精度最高的残差块（2个3x3卷积）
#### 2.增加通道数来加宽卷积层
- 主要的好处是：  
①在不用加深网络的情况下学习到更多特征。  
②提高特征重用效率，即前面层提取的特征被后续层有效利用的程度。resnet用shortcut实现每一层只需要学习跟上一层的差值，可能导致很多层没学到新东西，在划水，虽然深度增加，但效率很低。而增加通道数可以更有利于每一层学到新东西。
- 文章引入深度因子和宽度因子，通过实验选出了最优的组合。
#### 3.dropout插入残差块的卷积之间
- 更宽的网络更容易过拟合，因此需要dropout起正则化作用。
- 如果在最后加会太晚，在两个卷积层之间，防止后一个卷积记住前一个的特定模型，以此防止过拟合。
### 三、注意点
#### 1.pytorch官方将wideresnet放在resnet同文件中，使用resnet框架实现，其中两点未复现：  
①bn->relu->conv的顺序（用50层做一次对比实验显示，在自己收集的数据集上，这个顺序比传统的提升3.5%准确率）  
②不使用瓶颈层（文章中研究的都是浅层模型，官方实现深层模型使用了瓶颈层）
# DenseNet
### 一、网络架构
![Alt](https://i-blog.csdnimg.cn/blog_migrate/b626b419aadf20467aace9ce01efb483.png)
![Alt](https://i-blog.csdnimg.cn/blog_migrate/3e5439db041d9cedb31da90388f98061.png)  
配置：  
![Alt](https://i-blog.csdnimg.cn/blog_migrate/7cb083ed2b9779ac387973300edf0d3a.png)
### 二、重点
#### 1.采用密集连接
- 每层与所有前驱层直接连接，每层输入为所有前驱层特征图的拼接，输出传递给所有后继层。
- 主要好处是：  
①大幅缓解梯度消失问题，浅层也有多条路径可以回传，使网络更稳定。
②优化信息传递，特征不需要逐层传递，而是直接被后续层引用，避免在传递过程中被污染。
③每一层都参与学习，减少冗余，在resnet中，可能一些层学不到新东西划水，但densenet强迫每一层都要学新东西。
#### 2.采用特征拼接代替求和
- 将特征沿通道维度拼接，保留所有原始特征。
- 相较于resnet的求和不增加通道数，特征拼接将浅层的特征直接拿来给后续层用，防止浅层信息缺失污染；由于是每一层都需要将学到的新东西拼接进去，即使学到的很少，也会在反向传播当中造成影响，不存在划水现象。
- 参数量少，每一层固定学习量，即channel=32，不用像其他网络一样设置很多通道数，同时还用了1x1卷积降维。
#### 3.过渡层
- 每一层输出的尺寸不同，因此将整个网络划分为多个block，每个之间加上过渡层用来匹配输入输出尺寸。
- 引入一个压缩因子，防止通道数增长太过庞大。
### 三、注意点
#### 1.pytorch官方未实现DenseNet264,新加了一个DenseNet161，其中的growth_rate=48。官方默认设置dropout(p=0),也就是不启用，因为bn已经有很强的正则化。在自己数据集上实验显示161效果最好，但都体现出过拟合现象（？）
# Xception
### 0.前期工作
#### 1.做了什么，为什么做？
- 极端的inception架构，将inception模块重新定义为普通卷积层和深度可分离卷积层的中间步骤（什么意思？）
- 与inception_v3参数差不多，但效果更好。
#### 2.亮点
- 重新定位inception与可分离卷积之间的关系
- 全架构基于深度可分离卷积层构建
#### 3.搞懂
- 深度可分离卷积是什么？提出的“相关性完全解耦”是什么意思？  
深度卷积：用于提取空间特征，channel不变，H/W改变。  
逐点卷积：用于提取通道特征，H/W不变，channel改变。  
先深度卷积然后再逐点卷积，让参数同时只用关注一个维度的信息（相关性完全解耦），减少构建空间与通道相关性的参数运算，提高参数利用率。  
例如3通道图像，可能红绿通道对应树叶特征，与空间无关，但普通卷积会强行关联它们，造成了无关参数增加。
- 这样的架构为什么有用？  
inception与普通卷积的差别在于它将通道分为3-4个独立段，用不同卷积核处理通道相关性，虽能减少冗余，但仍存在“多通道共享部分空间参数的问题”，全部替换为深度可分离卷积后，一个卷积核只用处理一个通道。  
# ResNeXt
### 0.前期工作
#### 1.做了什么，为什么做？
- 现有网络痛点：  
深度层面：vgg结构简洁，但会出现梯度消失问题；resnet解决了这个问题但是参数量巨大
宽度层面：inception采用多分支结构但每个都要定制卷积核数量、尺寸，不方便训练新的数据
- 目的是搭建一种高度模块化，超参数少，保持原有性能的网络，并研究cardinality对模型性能的提升
- 在resnet基础上做优化，简单理解就是将输入通道分为许多并行的小通道通过相同处理，然后连接，最后与原始输入做残差连接。
#### 2.亮点
- resnet基础上设计了简洁模块化的架构
- 提出三种等效的模块形式
#### 3.理解
- cardinality是什么？ $\color{green}{\checkmark}$  
cardinality（基数），简单理解就是通道分开的组数量。  
为什么提出它？ $\color{green}{\checkmark}$  
resnet参数量大，如果增加宽度和深度都会增加参数量，而基数不会提升参数量。  
为什么它比深度和宽度更能提升模型性能？ $\color{green}{\checkmark}$  
基数代表并行变换的数量，每个并行的分支具有相同的拓扑结构，能从不同角度学习特征，而宽度仅仅是增加卷积核数量，提取特征相对单一；深度会导致梯度消失等问题。
- 如何理解三种等效的模块形式？优缺点分别是什么？ $\color{green}{\checkmark}$  
①聚合残差变换（直接对多个变换结果求和）  
优点：理解简单，变换路径清晰 缺点：需要显示定义多个分支（branch1,branch2...），计算效率不够高。  
②早期拼接（变换前拼接输入，再通过卷积融合）  
优点：拼接+单卷积，一定程度简化分支管理 缺点：拼接操作可能带来额外开销。  
③分组卷积（通过分组卷积实现聚合变换）  
优点：最简洁，计算效率最高（文章中首选）